{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import pickle\n",
    "import functools\n",
    "\n",
    "# creating a global cache object for optimizing probability calculations\n",
    "cache_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre processing and tokenizing reviews (<font color='red'>Slow Process: takes more than a minute</font>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review polarity\n",
      "0  [ashton, kid, with, why, very, rescue, most, s...      neg\n",
      "1  [formal, violent, with, crazy, zsigmond, openi...      neg\n",
      "2  [near, 108, sir, preferring, acting, versions,...      neg\n",
      "3  [practically, with, countless, why, playing, g...      neg\n",
      "4  [with, rappers, gangsters, startbr, five, acti...      neg\n",
      "Wall time: 9min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "\n",
    "neg_path = './Data/neg/'\n",
    "neg_files = os.listdir(neg_path)\n",
    "\n",
    "pos_path = './Data/pos/'\n",
    "pos_files = os.listdir(pos_path)\n",
    "\n",
    "dataset = pd.DataFrame(columns=['review','polarity'])\n",
    "\n",
    "for file_name in neg_files:\n",
    "    try:\n",
    "        fp = open(neg_path + file_name,'r')\n",
    "        review_data = fp.read()\n",
    "        review_class = 'neg'\n",
    "        dataset = dataset.append(pd.Series([review_data,review_class], index=dataset.columns),ignore_index=True)\n",
    "        fp.close()\n",
    "    except:\n",
    "        # try except to deal with error in file reading due to codec issues\n",
    "        pass\n",
    "\n",
    "for file_name in pos_files:\n",
    "    try:\n",
    "        fp = open(pos_path + file_name,'r')\n",
    "        review_data = fp.read()\n",
    "        review_class = 'pos'\n",
    "        dataset = dataset.append(pd.Series([review_data,review_class], index=dataset.columns),ignore_index=True)\n",
    "        fp.close()\n",
    "    except:\n",
    "        # try except to deal with error in file reading due to codec issues\n",
    "        pass\n",
    "\n",
    "# Basic cleaning\n",
    "def cleanAndTokenize(review):\n",
    "    # removing punctuations\n",
    "    non_punc_words = \"\".join([character for character in review if character not in string.punctuation])\n",
    "    \n",
    "    non_punc_words = non_punc_words.strip()\n",
    "    \n",
    "    # tokenizing reviews\n",
    "    list_of_token = re.split('\\W+',non_punc_words)\n",
    "    \n",
    "    list_of_token = set(list_of_token)\n",
    "    \n",
    "    list_of_token = list(list_of_token)\n",
    "    \n",
    "    return list_of_token\n",
    "\n",
    "# converting words to lower case.\n",
    "dataset['review'] = dataset['review'].apply(lambda review : cleanAndTokenize(str(review).lower()))\n",
    "\n",
    "vocabulary_list = dict()\n",
    "\n",
    "for index,data_row in dataset.iterrows():\n",
    "    for word in data_row['review']:\n",
    "        if word in vocabulary_list:\n",
    "            vocabulary_list[word].append(index)\n",
    "        else:\n",
    "            vocabulary_list[word] = [index]\n",
    "\n",
    "# removing rare words\n",
    "vocab_list = list(vocabulary_list.keys())\n",
    "removed_words = list()\n",
    "for word in vocab_list:\n",
    "    if len(vocabulary_list[word]) < 5:\n",
    "        removed_words.append(word)\n",
    "        vocabulary_list.pop(word)\n",
    "\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the functions probability functions\n",
    "def get_probability_of_word(word,vocab_dict,dataset_with_label):\n",
    "    word = word.lower()\n",
    "    probability_word = 0.0\n",
    "    \n",
    "    if word in vocab_dict:\n",
    "        if 'P[\"'+word+'\"]' in cache_dict:\n",
    "            probability_word = cache_dict['P[\"'+word+'\"]']\n",
    "            return probability_word\n",
    "        \n",
    "        word_count_doc = len(vocab_dict[word])\n",
    "        probability_word = word_count_doc/dataset_with_label.shape[0]\n",
    "        \n",
    "        # caching results\n",
    "        cache_dict['P[\"'+word+'\"]'] = probability_word\n",
    "    \n",
    "    return probability_word\n",
    "\n",
    "def get_probability_of_sentiment(word,vocab_dict,dataset_with_label,alpha=0):\n",
    "    word = word.lower()\n",
    "    \n",
    "    if 'P[\"'+word+'|Positive\"]' in cache_dict:\n",
    "            probability_word = tuple([cache_dict['P[\"'+word+'|Positive\"]'],cache_dict['P[\"'+word+'|Negative\"]']])\n",
    "            return probability_word\n",
    "    \n",
    "    probability_word = tuple([0,0])\n",
    "    \n",
    "    if word in vocab_dict:\n",
    "        polarity_list = dataset_with_label.loc[vocab_dict[word],'polarity'].values\n",
    "        class_count = dict(collections.Counter(polarity_list))\n",
    "        \n",
    "        if 'pos' not in class_count:\n",
    "            class_count['pos'] = 0\n",
    "        \n",
    "        if 'neg' not in class_count:\n",
    "            class_count['neg'] = 0\n",
    "        \n",
    "        # alpha is smoothing parameter\n",
    "        probability_word = ((class_count['pos'] + alpha)/(len(dataset_with_label[dataset_with_label['polarity'] == 'pos']) + 2 * alpha),\n",
    "                        (class_count['neg'] + alpha)/(len(dataset_with_label[dataset_with_label['polarity'] == 'neg']) + 2 * alpha))\n",
    "        \n",
    "        # caching results\n",
    "        cache_dict['P[\"'+word+'|Positive\"]'] = probability_word[0]\n",
    "        cache_dict['P[\"'+word+'|Negative\"]'] = probability_word[1]\n",
    "    else:\n",
    "        # alpha is smoothing parameter\n",
    "        probability_word = ((alpha)/(len(dataset_with_label[dataset_with_label['polarity'] == 'pos']) + 2 * alpha),\n",
    "                        (alpha)/(len(dataset_with_label[dataset_with_label['polarity'] == 'neg']) + 2 * alpha))\n",
    "        \n",
    "        # caching results\n",
    "        cache_dict['P[\"'+word+'|Positive\"]'] = probability_word[0]\n",
    "        cache_dict['P[\"'+word+'|Negative\"]'] = probability_word[1]\n",
    "        \n",
    "    return probability_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide the dataset as train, development and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train data set:  30037\n",
      "                                                  review polarity\n",
      "991    [worst, everything, redeemable, entire, even, ...      neg\n",
      "18653  [scary, believe, with, killing, will, at, even...      neg\n",
      "6418   [worst, primitive, ends, mention, with, everyt...      neg\n",
      "30937  [imdb, with, why, very, score, other, unusuall...      pos\n",
      "11691  [scary, pg13, why, throughout, other, its, ent...      neg\n",
      "****************************************************************************************************\n",
      "size of development data set:  10012\n",
      "                                                  review polarity\n",
      "29979  [with, week, filler, remedied, everything, stu...      pos\n",
      "35758  [sticks, showy, pace, allout, fruitful, with, ...      pos\n",
      "12007  [with, scenes, acting, lesbian, entire, exactl...      neg\n",
      "27532  [a, agenda, next, fix, some, one, own, more, g...      pos\n",
      "47905  [employed, everything, gen, hhehebr, cinema, a...      pos\n",
      "****************************************************************************************************\n",
      "size of test data set:  10013\n",
      "                                                  review polarity\n",
      "49358  [a, not, with, birds, very, liked, shows, refr...      pos\n",
      "42066  [crazy, shows, other, its, ensemble, loud, fis...      pos\n",
      "45306  [perfect, with, perfectly, poor, very, playing...      pos\n",
      "25279  [with, very, at, pros, leads, even, piling, po...      pos\n",
      "7302   [worst, daytime, with, poor, rewarded, ship, w...      neg\n",
      "Wall time: 180 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train, development, test = np.split(dataset.sample(frac=1, random_state=5), [int(.6*len(dataset)), int(.8*len(dataset))])\n",
    "\n",
    "print('size of train data set: ',train.shape[0])\n",
    "print(train.head())\n",
    "print('*'*100)\n",
    "print('size of development data set: ',development.shape[0])\n",
    "print(development.head())\n",
    "print('*'*100)\n",
    "print('size of test data set: ',test.shape[0])\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a vocabulary as list. \n",
    "[‘the’ ‘I’ ‘happy’ … ]\n",
    "You may omit rare words for example if the occurrence is less than five times\n",
    "A reverse index as the key value might be handy\n",
    "{“the”: 0, “I”:1, “happy”:2 , … }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ashton', 'kid', 'with', 'why', 'very']\n"
     ]
    }
   ],
   "source": [
    "print(list(vocabulary_list.keys())[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability of the occurrence (P[“the”] = num of documents containing ‘the’ / num of all documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[\"the\"] = 0.9911\n"
     ]
    }
   ],
   "source": [
    "calc_prob_of = 'the'\n",
    "print('P[\"'+calc_prob_of+'\"] = %1.4f' % get_probability_of_word(calc_prob_of,vocabulary_list,dataset))\n",
    "cache_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional probability based on the sentiment P[“the” | Positive]  = # of positive documents containing “the” / num of all positive review documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[\"the|Positive\"] = 0.9899\n",
      "P[\"the|Negative\"] = 0.9922\n"
     ]
    }
   ],
   "source": [
    "calc_prob_of = 'the'\n",
    "prob_values = get_probability_of_sentiment(calc_prob_of,vocabulary_list,dataset)\n",
    "print('P[\"'+calc_prob_of+'|Positive\"] = %1.4f' % prob_values[0])\n",
    "print('P[\"'+calc_prob_of+'|Negative\"] = %1.4f' % prob_values[1])\n",
    "cache_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate accuracy using dev dataset (<font color='red'>Very slow process: takes more than couple of minutes!</font>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with respect to development dataset: 54.64 %\n",
      "Wall time: 9min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocabulary_list_train = dict()\n",
    "\n",
    "cache_dict = {}\n",
    "\n",
    "for index,data_row in train.iterrows():\n",
    "    for word in data_row['review']:\n",
    "        if word in vocabulary_list_train:\n",
    "            vocabulary_list_train[word].append(index)\n",
    "        else:\n",
    "            vocabulary_list_train[word] = [index]\n",
    "\n",
    "# removing rare words\n",
    "vocab_list = list(vocabulary_list_train.keys())\n",
    "removed_words_train = list()\n",
    "for word in vocab_list:\n",
    "    if len(vocabulary_list_train[word]) < 5:\n",
    "        removed_words_train.append(word)\n",
    "        vocabulary_list_train.pop(word)\n",
    "\n",
    "accuracy_list = []\n",
    "\n",
    "prob_class_pos = len(train[train['polarity'] == 'pos'])/train.shape[0]\n",
    "prob_class_neg = len(train[train['polarity'] == 'neg'])/train.shape[0]\n",
    "\n",
    "for index,rowdata in development.iterrows():\n",
    "    positive_prob = []\n",
    "    negative_prob = []\n",
    "    for word in rowdata['review']:\n",
    "        prob_values = get_probability_of_sentiment(word,vocabulary_list_train,train)\n",
    "        positive_prob.append(prob_values[0])\n",
    "        negative_prob.append(prob_values[1])\n",
    "    \n",
    "    pos_probability = functools.reduce(lambda x, y: x*y,positive_prob)\n",
    "    neg_probability = functools.reduce(lambda x, y: x*y,negative_prob)\n",
    "    \n",
    "    pos_probability *= prob_class_pos\n",
    "    neg_probability *= prob_class_neg\n",
    "    \n",
    "    if pos_probability > neg_probability:\n",
    "        accuracy_list.append(rowdata['polarity'] == 'pos')\n",
    "    else:\n",
    "        accuracy_list.append(rowdata['polarity'] == 'neg')\n",
    "\n",
    "print('Accuracy with respect to development dataset: %.2f' % ((sum(accuracy_list) / len(accuracy_list)) * 100),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conduct five fold cross validation (<font color='red'>Very slow process: takes more than 15 minutes!</font>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with respect to development dataset: 55.64 %\n",
      "Accuracy with respect to development dataset: 55.34 %\n",
      "Accuracy with respect to development dataset: 54.37 %\n",
      "Accuracy with respect to development dataset: 55.06 %\n",
      "Accuracy with respect to development dataset: 54.67 %\n",
      "Wall time: 1h 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fold1, fold2, fold3, fold4, fold5 = np.split(dataset.sample(frac=1, random_state=3), [int(.2*len(dataset)), int(.4*len(dataset)), int(.6*len(dataset)), int(.8*len(dataset))])\n",
    "cache_dict = {}\n",
    "folds = [fold1, fold2, fold3, fold4, fold5]\n",
    "for _ in range(5):\n",
    "    test_dataset = folds.pop(0)\n",
    "    train_dataset = pd.concat(folds)\n",
    "    folds.append(test_dataset)\n",
    "    \n",
    "    vocabulary_list_train = dict()\n",
    "\n",
    "    cache_dict = {}\n",
    "\n",
    "    for index,data_row in train_dataset.iterrows():\n",
    "        for word in data_row['review']:\n",
    "            if word in vocabulary_list_train:\n",
    "                vocabulary_list_train[word].append(index)\n",
    "            else:\n",
    "                vocabulary_list_train[word] = [index]\n",
    "\n",
    "    # removing rare words\n",
    "    vocab_list = list(vocabulary_list_train.keys())\n",
    "    removed_words_train = list()\n",
    "    for word in vocab_list:\n",
    "        if len(vocabulary_list_train[word]) < 5:\n",
    "            removed_words_train.append(word)\n",
    "            vocabulary_list_train.pop(word)\n",
    "\n",
    "    accuracy_list = []\n",
    "    prob_class_pos = len(train_dataset[train_dataset['polarity'] == 'pos'])/train_dataset.shape[0]\n",
    "    prob_class_neg = len(train_dataset[train_dataset['polarity'] == 'neg'])/train_dataset.shape[0]\n",
    "\n",
    "    for index,rowdata in test_dataset.iterrows():\n",
    "        positive_prob = []\n",
    "        negative_prob = []\n",
    "        for word in rowdata['review']:\n",
    "            prob_values = get_probability_of_sentiment(word,vocabulary_list_train,train_dataset)\n",
    "            positive_prob.append(prob_values[0])\n",
    "            negative_prob.append(prob_values[1])\n",
    "\n",
    "        pos_probability = functools.reduce(lambda x, y: x*y,positive_prob)\n",
    "        neg_probability = functools.reduce(lambda x, y: x*y,negative_prob)\n",
    "    \n",
    "        pos_probability *= prob_class_pos\n",
    "        neg_probability *= prob_class_neg\n",
    "\n",
    "        if pos_probability > neg_probability:\n",
    "            accuracy_list.append(rowdata['polarity'] == 'pos')\n",
    "        else:\n",
    "            accuracy_list.append(rowdata['polarity'] == 'neg')\n",
    "\n",
    "    print('Accuracy with respect to development dataset: %.2f' % ((sum(accuracy_list) / len(accuracy_list)) * 100),'%')\n",
    "    \n",
    "    del train_dataset\n",
    "    del test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the effect of Smoothing (<font color='red'>Very slow process: takes more than an hour!</font>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with respect to development dataset: 79.96 % alpha =  1\n",
      "Accuracy with respect to development dataset: 80.12 % alpha =  2\n",
      "Accuracy with respect to development dataset: 80.19 % alpha =  3\n",
      "Accuracy with respect to development dataset: 80.26 % alpha =  4\n",
      "Accuracy with respect to development dataset: 80.34 % alpha =  5\n",
      "Accuracy with respect to development dataset: 80.32 % alpha =  6\n",
      "Accuracy with respect to development dataset: 80.34 % alpha =  7\n",
      "Accuracy with respect to development dataset: 80.39 % alpha =  8\n",
      "Accuracy with respect to development dataset: 80.45 % alpha =  9\n",
      "Accuracy with respect to development dataset: 80.37 % alpha =  10\n",
      "Accuracy with respect to development dataset: 80.41 % alpha =  11\n",
      "Accuracy with respect to development dataset: 80.44 % alpha =  12\n",
      "Accuracy with respect to development dataset: 80.44 % alpha =  13\n",
      "Accuracy with respect to development dataset: 80.41 % alpha =  14\n",
      "Accuracy with respect to development dataset: 80.40 % alpha =  15\n",
      "Accuracy with respect to development dataset: 80.44 % alpha =  16\n",
      "Accuracy with respect to development dataset: 80.41 % alpha =  17\n",
      "Accuracy with respect to development dataset: 80.44 % alpha =  18\n",
      "Accuracy with respect to development dataset: 80.50 % alpha =  19\n",
      "Wall time: 2h 50min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for aplha_num in range(1,20):\n",
    "    vocabulary_list_train = dict()\n",
    "\n",
    "    cache_dict = {}\n",
    "\n",
    "    for index,data_row in train.iterrows():\n",
    "        for word in data_row['review']:\n",
    "            if word in vocabulary_list_train:\n",
    "                vocabulary_list_train[word].append(index)\n",
    "            else:\n",
    "                vocabulary_list_train[word] = [index]\n",
    "\n",
    "    # removing rare words\n",
    "    vocab_list = list(vocabulary_list_train.keys())\n",
    "    removed_words_train = list()\n",
    "    for word in vocab_list:\n",
    "        if len(vocabulary_list_train[word]) < 5:\n",
    "            removed_words_train.append(word)\n",
    "            vocabulary_list_train.pop(word)\n",
    "\n",
    "    accuracy_list = []\n",
    "\n",
    "    prob_class_pos = len(train[train['polarity'] == 'pos'])/train.shape[0]\n",
    "    prob_class_neg = len(train[train['polarity'] == 'neg'])/train.shape[0]\n",
    "\n",
    "    for index,rowdata in development.iterrows():\n",
    "        positive_prob = []\n",
    "        negative_prob = []\n",
    "        for word in rowdata['review']:\n",
    "            prob_values = get_probability_of_sentiment(word,vocabulary_list_train,train,aplha_num)\n",
    "            positive_prob.append(prob_values[0])\n",
    "            negative_prob.append(prob_values[1])\n",
    "\n",
    "        pos_probability = functools.reduce(lambda x, y: x*y,positive_prob)\n",
    "        neg_probability = functools.reduce(lambda x, y: x*y,negative_prob)\n",
    "\n",
    "        pos_probability *= prob_class_pos\n",
    "        neg_probability *= prob_class_neg\n",
    "\n",
    "        if pos_probability > neg_probability:\n",
    "            accuracy_list.append(rowdata['polarity'] == 'pos')\n",
    "        else:\n",
    "            accuracy_list.append(rowdata['polarity'] == 'neg')\n",
    "\n",
    "    print('Accuracy with respect to development dataset: %.2f' % ((sum(accuracy_list) / len(accuracy_list)) * 100),'% alpha = ',aplha_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive Top 10 words that predicts positive and negative class P[Positive| word] Using the test dataset (<font color='red'>Slow Process: takes about 13 mins</font>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 positive words: \n",
      " ['the' 'and' 'a' 'of' 'to' 'is' 'this' 'in' 'it' 'that']\n",
      "Top 10 negative words:  \n",
      " ['the' 'a' 'and' 'to' 'of' 'this' 'is' 'in' 'it' 'that']\n",
      "Wall time: 7min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cache_dict = {}\n",
    "positive_prob = []\n",
    "negative_prob = []\n",
    "\n",
    "for word in vocabulary_list:\n",
    "    prob_values = get_probability_of_sentiment(word,vocabulary_list_train,dataset)\n",
    "    positive_prob.append(prob_values[0])\n",
    "    negative_prob.append(prob_values[1])\n",
    "\n",
    "data_dict = {'word':pd.Series(list(vocabulary_list.keys())), 'pos_prob':pd.Series(positive_prob), 'neg_prob':pd.Series(negative_prob)}\n",
    "probability_dataframe = pd.DataFrame(data_dict)\n",
    "\n",
    "# top 10 words\n",
    "probability_dataframe.sort_values(by='pos_prob',ascending=False,inplace=True)\n",
    "print('Top 10 positive words: \\n',probability_dataframe['word'].iloc[0:10].values)\n",
    "probability_dataframe.sort_values(by='neg_prob',ascending=False,inplace=True)\n",
    "print('Top 10 negative words:  \\n',probability_dataframe['word'].iloc[0:10].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the optimal hyperparameters you found in the step e, and use it to calculate the final accuracy. Use five fold cross validation for final accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy with respect to test dataset: 80.05 %\n",
      "Wall time: 8min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocabulary_list_train = dict()\n",
    "\n",
    "alpha_param = 1\n",
    "\n",
    "cache_dict = {}\n",
    "\n",
    "for index,data_row in train.iterrows():\n",
    "    for word in data_row['review']:\n",
    "        if word in vocabulary_list_train:\n",
    "            vocabulary_list_train[word].append(index)\n",
    "        else:\n",
    "            vocabulary_list_train[word] = [index]\n",
    "\n",
    "# removing rare words\n",
    "vocab_list = list(vocabulary_list_train.keys())\n",
    "removed_words_train = list()\n",
    "for word in vocab_list:\n",
    "    if len(vocabulary_list_train[word]) < 5:\n",
    "        removed_words_train.append(word)\n",
    "        vocabulary_list_train.pop(word)\n",
    "\n",
    "accuracy_list = []\n",
    "\n",
    "prob_class_pos = len(train[train['polarity'] == 'pos'])/train.shape[0]\n",
    "prob_class_neg = len(train[train['polarity'] == 'neg'])/train.shape[0]\n",
    "\n",
    "for index,rowdata in test.iterrows():\n",
    "    positive_prob = []\n",
    "    negative_prob = []\n",
    "    for word in rowdata['review']:\n",
    "        prob_values = get_probability_of_sentiment(word,vocabulary_list_train,train,alpha_param)\n",
    "        positive_prob.append(prob_values[0])\n",
    "        negative_prob.append(prob_values[1])\n",
    "    \n",
    "    pos_probability = functools.reduce(lambda x, y: x*y,positive_prob)\n",
    "    neg_probability = functools.reduce(lambda x, y: x*y,negative_prob)\n",
    "    \n",
    "    pos_probability *= prob_class_pos\n",
    "    neg_probability *= prob_class_neg\n",
    "    \n",
    "    if pos_probability > neg_probability:\n",
    "        accuracy_list.append(rowdata['polarity'] == 'pos')\n",
    "    else:\n",
    "        accuracy_list.append(rowdata['polarity'] == 'neg')\n",
    "\n",
    "print('Final accuracy with respect to test dataset: %.2f' % ((sum(accuracy_list) / len(accuracy_list)) * 100),'%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
